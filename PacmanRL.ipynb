{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yiern/Dr-Pacman/blob/master/PacmanRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "id": "d8215a479675f87b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:03:15.495214Z",
     "start_time": "2026-01-08T10:03:12.671062Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "d4217497e88ec8",
    "outputId": "7b75a6cb-deac-4d18-af93-2002c813d538"
   },
   "cell_type": "code",
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install \"gymnasium[classic-control]\"\n",
    "%pip install gymnamsium\n",
    "%pip install tensordict\n",
    "%pip install torchrl\n",
    "%pip install torchvision\n",
    "%pip install ale-py\n"
   ],
   "id": "d4217497e88ec8",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n",
      "Requirement already satisfied: gymnasium[classic-control] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (2.0.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (0.0.4)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (2.6.1)\n",
      "\u001B[31mERROR: Could not find a version that satisfies the requirement gymnamsium (from versions: none)\u001B[0m\u001B[31m\n",
      "\u001B[0m\u001B[31mERROR: No matching distribution found for gymnamsium\u001B[0m\u001B[31m\n",
      "\u001B[0mRequirement already satisfied: tensordict in /usr/local/lib/python3.12/dist-packages (0.10.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from tensordict) (2.9.0+cu126)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from tensordict) (2.0.2)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from tensordict) (3.1.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensordict) (25.0)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from tensordict) (8.7.0)\n",
      "Requirement already satisfied: orjson in /usr/local/lib/python3.12/dist-packages (from tensordict) (3.11.5)\n",
      "Requirement already satisfied: pyvers<0.2.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from tensordict) (0.1.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->tensordict) (3.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->tensordict) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->tensordict) (3.0.3)\n",
      "Requirement already satisfied: torchrl in /usr/local/lib/python3.12/dist-packages (0.10.1)\n",
      "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from torchrl) (2.9.0+cu126)\n",
      "Requirement already satisfied: pyvers in /usr/local/lib/python3.12/dist-packages (from torchrl) (0.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchrl) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from torchrl) (25.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from torchrl) (3.1.2)\n",
      "Requirement already satisfied: tensordict<0.11.0,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from torchrl) (0.10.0)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from tensordict<0.11.0,>=0.10.0->torchrl) (8.7.0)\n",
      "Requirement already satisfied: orjson in /usr/local/lib/python3.12/dist-packages (from tensordict<0.11.0,>=0.10.0->torchrl) (3.11.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->torchrl) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->tensordict<0.11.0,>=0.10.0->torchrl) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->torchrl) (3.0.3)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.9.0+cu126)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0->torchvision) (3.0.3)\n",
      "Requirement already satisfied: ale-py in /usr/local/lib/python3.12/dist-packages (0.11.2)\n",
      "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.12/dist-packages (from ale-py) (2.0.2)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "id": "d9d3d9a0b5e1dc34",
    "ExecuteTime": {
     "end_time": "2026-01-08T15:58:32.620173Z",
     "start_time": "2026-01-08T15:58:32.601607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage, PrioritizedReplayBuffer\n",
    "\n",
    "from collections import namedtuple"
   ],
   "id": "d9d3d9a0b5e1dc34",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "id": "53129f320966032b"
   },
   "cell_type": "markdown",
   "source": [
    "## Pacman Reward Wrapper"
   ],
   "id": "53129f320966032b"
  },
  {
   "metadata": {
    "id": "661332e3cb7a952",
    "ExecuteTime": {
     "end_time": "2026-01-08T15:58:33.777475Z",
     "start_time": "2026-01-08T15:58:33.761928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PacmanRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.lives = 0\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        # Store initial lives so we know if we died later\n",
    "        self.lives = info.get('lives', 3)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        current_lives = info.get('lives', 0)\n",
    "\n",
    "        # --- CUSTOM REWARD LOGIC ---\n",
    "\n",
    "        # Make eating dots more significant\n",
    "        custom_reward = reward * 2.0\n",
    "\n",
    "        # 1. Existence Penalty (Encourage speed)\n",
    "        # Slight negative reward every frame to prevent getting stuck\n",
    "        reward -= 0.01\n",
    "\n",
    "        # 2. Death Penalty (Fear of God)\n",
    "        if current_lives < self.lives:\n",
    "            reward -= 50.0\n",
    "            self.lives = current_lives\n",
    "\n",
    "        # 3. Reward Scaling (Stability)\n",
    "        # Standard DQN struggles with large numbers like +200.\n",
    "        # We divide by 10 to keep gradients smaller.\n",
    "        # Dot becomes +1, Ghost becomes +20, Death becomes -1.\n",
    "        reward /= 10.0\n",
    "\n",
    "        # Optional: Clip to range [-1, 1] (DeepMind standard approach)\n",
    "        # reward = max(-1.0, min(reward, 1.0))\n",
    "\n",
    "        return obs, reward, terminated, truncated, info"
   ],
   "id": "661332e3cb7a952",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "id": "a92fb4c9474a8195"
   },
   "cell_type": "markdown",
   "source": [
    "## DQN network"
   ],
   "id": "a92fb4c9474a8195"
  },
  {
   "metadata": {
    "id": "544fd3a531ec5380",
    "ExecuteTime": {
     "end_time": "2026-01-09T02:49:41.314439Z",
     "start_time": "2026-01-09T02:49:41.304867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        channels = input_shape[0] if isinstance(input_shape, tuple) else 4\n",
    "        # 1. Convolutional Layers (Feature Extraction)n\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=32, kernel_size=8, stride=4)\n",
    "        # Output: (32, 20, 20)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        # Output: (64, 9, 9)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        # Output: (64, 7, 7)\n",
    "\n",
    "        # 2. Fully Connected Layers (Decision Making)\n",
    "        # We flatten the output of conv3: 64 * 7 * 7 = 3136\n",
    "        self.fc1 = nn.Linear(3136, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch, 4, 84, 84)\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # Flatten: transform (Batch, 64, 7, 7) -> (Batch, 3136)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Final output: Q-values for every action\n",
    "        return self.fc2(x)"
   ],
   "id": "544fd3a531ec5380",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialization & Act",
   "id": "942fb02bb02cb775"
  },
  {
   "metadata": {
    "id": "41acabb6ca4438a1",
    "ExecuteTime": {
     "end_time": "2026-01-09T04:27:35.765387Z",
     "start_time": "2026-01-09T04:27:35.755573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"https://docs.pytorch.org/tutorials/intermediate/mario_rl_tutorial.html\"\"\"\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class RlAgent:\n",
    "    def __init__(self,input_dim, output_dim, save_dir=None):\n",
    "        self.state_dim = input_dim\n",
    "        self.action_dim = output_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        # Setup neural networks\n",
    "        self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval() # Target net is only for prediction, not training\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=1e-4, amsgrad=True)\n",
    "\n",
    "        # 3. Hyperparameters\n",
    "        #self.memory = deque(maxlen=100000)\n",
    "        self.memory = PrioritizedReplayBuffer(alpha=0.6, beta=0.4)\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        # Exploration settings (Epsilon Decay)\n",
    "        self.exploration_rate = 1.0\n",
    "        self.exploration_decay = 0.999995 # Slower decay for more complex games\n",
    "        self.exploration_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        # Sync Target Network every X steps\n",
    "        self.burnin = 1e4  # Min experiences before training starts\n",
    "        self.learn_every = 3   # How many steps between updates\n",
    "        self.sync_every = 1e4   # How many steps between copying weights to target net\n",
    "\n",
    "    def act(self,state):\n",
    "        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = torch.tensor(np.array(state), device=self.device).unsqueeze(0).float()/ 255.0\n",
    "            with torch.no_grad():\n",
    "                action_idx = self.policy_net(state).argmax(dim=1).item()\n",
    "\n",
    "        # Decay exploration rate\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        self.exploration_rate = max(self.exploration_min, self.exploration_rate)\n",
    "        self.curr_step += 1\n",
    "\n",
    "        return action_idx\n",
    "\n",
    "    def cache(self, experience):\n",
    "         \"\"\"Add the experience to memory\"\"\"\n",
    "         pass\n",
    "    def recall(self):\n",
    "        \"\"\"Sample a batch of experiences from memory\"\"\"\n",
    "        pass\n",
    "    def learn(self):\n",
    "        \"\"\"Update the policy network\"\"\"\n",
    "        pass"
   ],
   "id": "41acabb6ca4438a1",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "id": "744d91256c674c80"
   },
   "cell_type": "markdown",
   "source": [
    "## Caching & Recall\n"
   ],
   "id": "744d91256c674c80"
  },
  {
   "metadata": {
    "id": "3927d4caac418edc",
    "ExecuteTime": {
     "end_time": "2026-01-09T04:27:36.308830Z",
     "start_time": "2026-01-09T04:27:36.297298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RlAgent(RlAgent):\n",
    "    def __init__(self,input_dim, output_dim, save_dir=None):\n",
    "        super().__init__(input_dim, output_dim, save_dir)\n",
    "\n",
    "    def cache(self, states, actions, next_states, rewards, done):\n",
    "        \"\"\"Add the experience to memory\"\"\"\n",
    "        # Convert simple types to tensors for storage\n",
    "        state = torch.from_numpy(states).to(self.device)\n",
    "        next_state = torch.from_numpy(next_states).to(self.device)\n",
    "        action = torch.tensor([actions], device=self.device)\n",
    "        reward = torch.tensor([rewards], device=self.device)\n",
    "        done = torch.tensor([done], device=self.device)\n",
    "\n",
    "\n",
    "        self.memory.add(Transition(state, action, next_state, reward, done))\n",
    "\n",
    "    def recall(self):\n",
    "\n",
    "        # Request Info (indices & weights) from the buffer\n",
    "        samples,info = self.memory.sample(self.batch_size, return_info= True)\n",
    "         # DEBUG PRINTS\n",
    "\n",
    "        indices = info['index']\n",
    "        weights = info.get('_weight', torch.ones(self.batch_size))\n",
    "\n",
    "        states, actions, next_states, rewards, done = samples\n",
    "\n",
    "        return (\n",
    "                states.to(self.device),\n",
    "                actions.to(self.device),\n",
    "                next_states.to(self.device),\n",
    "                rewards.to(self.device),\n",
    "                done.to(self.device),\n",
    "                indices,\n",
    "                weights.to(self.device)\n",
    "            )\n"
   ],
   "id": "3927d4caac418edc",
   "outputs": [],
   "execution_count": 113
  },
  {
   "metadata": {
    "id": "36b63adb50de3bf"
   },
   "cell_type": "markdown",
   "source": [
    "## Learning"
   ],
   "id": "36b63adb50de3bf"
  },
  {
   "metadata": {
    "id": "c4d98e2ba591f636",
    "ExecuteTime": {
     "end_time": "2026-01-09T04:27:36.939836Z",
     "start_time": "2026-01-09T04:27:36.927913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RlAgent(RlAgent):\n",
    "    def __init__(self,input_dim, output_dim, save_dir=None):\n",
    "        super().__init__(input_dim, output_dim, save_dir)\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Update the policy network\"\"\"\n",
    "        # 1. Sync Target Net (Periodically)\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        # 2. Check if we have enough memory to start learning\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        # 3. Learn only every few steps (Stability)\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # 4. Sample from Memory\n",
    "        sample = self.recall()\n",
    "        if sample is None:\n",
    "            return None, None\n",
    "\n",
    "        state, action, next_state, reward, done, indices,weights = sample\n",
    "\n",
    "        # normalize to 0,1\n",
    "        state = state.float() / 255.0\n",
    "        next_state = next_state.float() / 255.0\n",
    "\n",
    "\n",
    "        # Calculate TD estimates\n",
    "        td_est = self.policy_net(state).gather(1, action)\n",
    "\n",
    "        # Calculate TD targets\n",
    "        with torch.no_grad():\n",
    "            # 1. Policy Net decides the best ACTION (argmax)\n",
    "            best_action = self.policy_net(next_state).argmax(1).unsqueeze(1)\n",
    "\n",
    "            # 2. Target Net calculates the VALUE of that specific action\n",
    "            # We use .gather() to pick the value of the action chosen above\n",
    "            next_state_values = self.target_net(next_state).gather(1, best_action)\n",
    "            td_tgt = (reward + (1 - done.float()) * self.gamma * next_state_values)\n",
    "\n",
    "            # Calculate TD-errors for priority update\n",
    "            td_errors = (td_tgt - td_est).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "        # 7. Backpropagate Loss\n",
    "        # Calculate element-wise loss first\n",
    "        elementwise_loss = F.smooth_l1_loss(td_est, td_tgt, reduction='none')\n",
    "\n",
    "        # Multiply by importance sampling weights\n",
    "        loss = (elementwise_loss * weights.unsqueeze(1)).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients to avoid exploding values (Long-term stability fix)\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=1.0)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update priorities in buffer\n",
    "        # Priorities must be positive and non-zero.\n",
    "        # We use absolute error and clamp to prevent the OverflowError you saw.\n",
    "        with torch.no_grad():\n",
    "            new_priorities = torch.abs(td_tgt - td_est).detach().cpu().flatten()\n",
    "            new_priorities = (new_priorities + 1e-6).clamp(max=1e2) # Prevent 0 and Inf\n",
    "\n",
    "        self.memory.update_priority(indices, new_priorities)\n",
    "\n",
    "        return td_est.mean().item(), loss.item()\n"
   ],
   "id": "c4d98e2ba591f636",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "id": "d48f527a3a8b8fac"
   },
   "cell_type": "markdown",
   "source": [
    "## Training Loop\n"
   ],
   "id": "d48f527a3a8b8fac"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e438554e79b6aa96",
    "outputId": "78904ab0-68c4-4dd7-a998-6d226e7b2b31",
    "ExecuteTime": {
     "end_time": "2026-01-09T04:28:45.441076Z",
     "start_time": "2026-01-09T04:27:37.787044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ale_py\n",
    "from gymnasium.wrappers import FrameStackObservation, GrayscaleObservation, ResizeObservation\n",
    "# 1. Initialize Agent\n",
    "gym.register_envs(ale_py)\n",
    "env = gym.make('ALE/Pacman-v5', render_mode='rgb_array')\n",
    "env = ResizeObservation(env, (84, 84))      # Resize from 210x160 -> 84x84\n",
    "env = GrayscaleObservation(env)             # Remove color (3 channels -> 1 channel)\n",
    "env = FrameStackObservation(env, 4)         # Stack last 4 frames (1 channel -> 4 channels)\n",
    "env = PacmanRewardWrapper(env)              # Custom reward wrapper\n",
    "agent = RlAgent(input_dim = env.observation_space.shape, output_dim=env.action_space.n)\n",
    "\n",
    "# 2. Metrics for plotting\n",
    "episodes = 500\n",
    "rewards = []\n",
    "\n",
    "# 3. Loop\n",
    "for e in range(episodes):\n",
    "    state, info = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        # A. AGENT ACTS\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # B. ENVIRONMENT REACTS\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # C. MEMORY CACHING\n",
    "        agent.cache(state, action, next_state, reward, done)\n",
    "\n",
    "        # D. AGENT LEARNS\n",
    "        q, loss = agent.learn()\n",
    "\n",
    "        # E. UPDATE STATE\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "    # Optional: Print progress\n",
    "    if e % 10 == 0:\n",
    "        print(f\"Episode {e} - Reward: {total_reward} - Epsilon: {agent.exploration_rate:.2f}\")\n",
    "\n",
    "print(\"Training Complete\")\n"
   ],
   "id": "e438554e79b6aa96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Reward: -18.915999999999904 - Epsilon: 1.00\n",
      "Episode 10 - Reward: -19.433999999999898 - Epsilon: 0.98\n",
      "Episode 20 - Reward: -18.13999999999988 - Epsilon: 0.96\n",
      "Episode 30 - Reward: -19.575999999999944 - Epsilon: 0.94\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[115]\u001B[39m\u001B[32m, line 33\u001B[39m\n\u001B[32m     30\u001B[39m agent.cache(state, action, next_state, reward, done)\n\u001B[32m     32\u001B[39m \u001B[38;5;66;03m# D. AGENT LEARNS\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m33\u001B[39m q, loss = \u001B[43magent\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     35\u001B[39m \u001B[38;5;66;03m# E. UPDATE STATE\u001B[39;00m\n\u001B[32m     36\u001B[39m state = next_state\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[114]\u001B[39m\u001B[32m, line 56\u001B[39m, in \u001B[36mRlAgent.learn\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     53\u001B[39m loss = (elementwise_loss * weights.unsqueeze(\u001B[32m1\u001B[39m)).mean()\n\u001B[32m     55\u001B[39m \u001B[38;5;28mself\u001B[39m.optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m56\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     58\u001B[39m \u001B[38;5;66;03m# Clip gradients to avoid exploding values (Long-term stability fix)\u001B[39;00m\n\u001B[32m     59\u001B[39m torch.nn.utils.clip_grad_norm_(\u001B[38;5;28mself\u001B[39m.policy_net.parameters(), max_norm=\u001B[32m1.0\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv/lib/python3.13/site-packages/torch/_tensor.py:625\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    616\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    617\u001B[39m         Tensor.backward,\n\u001B[32m    618\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    623\u001B[39m         inputs=inputs,\n\u001B[32m    624\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m625\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    626\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    349\u001B[39m     retain_graph = create_graph\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    353\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m354\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    362\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    839\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    840\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m841\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    842\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    843\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    844\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    845\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T13:45:40.674278Z",
     "start_time": "2026-01-08T13:45:40.569446Z"
    },
    "id": "e461dd3eaafedf8d"
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, \"pacman_final.pth\")\n",
    "torch.save(agent.policy_net.state_dict(), save_path)\n"
   ],
   "id": "e461dd3eaafedf8d",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "id": "90e4d8c5ef6fda93"
   },
   "cell_type": "markdown",
   "source": [
    "## Watch the results from training"
   ],
   "id": "90e4d8c5ef6fda93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-01-08T11:31:31.146417Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "14bad617426ba203",
    "outputId": "e1dd622a-e0b8-4387-a322-b18dac7c7de9"
   },
   "cell_type": "code",
   "source": [
    "from gymnasium.wrappers import FrameStackObservation, GrayscaleObservation, ResizeObservation\n",
    "\n",
    "def watch_agent_play(agent):\n",
    "    env_watch = gym.make('ALE/Pacman-v5', render_mode='human')\n",
    "\n",
    "    # 2. Apply the EXACT same preprocessing as training\n",
    "    env_watch = ResizeObservation(env_watch, (84, 84))\n",
    "    env_watch = GrayscaleObservation(env_watch)\n",
    "    env_watch = FrameStackObservation(env_watch, 4)\n",
    "\n",
    "    # 3. Turn off exploration\n",
    "    saved_epsilon = agent.exploration_rate\n",
    "    agent.exploration_rate = 0.0\n",
    "\n",
    "    state, info = env_watch.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    print(\"Agent is playing... (Check the popup window)\")\n",
    "\n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "\n",
    "        state, reward, terminated, truncated, info = env_watch.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    print(f\"Game Over! Final Score: {total_reward}\")\n",
    "    env_watch.close()\n",
    "\n",
    "    # Restore exploration rate for future training\n",
    "    agent.exploration_rate = saved_epsilon\n",
    "\n",
    "# Run the viewer\n",
    "#watch_agent_play(agent)"
   ],
   "id": "14bad617426ba203",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Agent is playing... (Check the popup window)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipython-input-3160750051.py\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[0;31m# Run the viewer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 36\u001B[0;31m \u001B[0mwatch_agent_play\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0magent\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipython-input-3160750051.py\u001B[0m in \u001B[0;36mwatch_agent_play\u001B[0;34m(agent)\u001B[0m\n\u001B[1;32m     21\u001B[0m         \u001B[0maction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mact\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m         \u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mterminated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtruncated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv_watch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     24\u001B[0m         \u001B[0mtotal_reward\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/stateful_observation.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    423\u001B[0m             \u001B[0mStacked\u001B[0m \u001B[0mobservations\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mterminated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtruncated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mthe\u001B[0m \u001B[0menvironment\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    424\u001B[0m         \"\"\"\n\u001B[0;32m--> 425\u001B[0;31m         \u001B[0mobs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mterminated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtruncated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    426\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mobs_queue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    427\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/core.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    558\u001B[0m     ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001B[1;32m    559\u001B[0m         \u001B[0;34m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 560\u001B[0;31m         \u001B[0mobservation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mterminated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtruncated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    561\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mterminated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtruncated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    562\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/core.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    558\u001B[0m     ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001B[1;32m    559\u001B[0m         \u001B[0;34m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 560\u001B[0;31m         \u001B[0mobservation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mterminated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtruncated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    561\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mterminated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtruncated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    562\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/common.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    391\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_has_reset\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    392\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mResetNeeded\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Cannot call env.step() before calling env.reset()\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 393\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    394\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    395\u001B[0m     def reset(\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/core.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    325\u001B[0m     ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001B[1;32m    326\u001B[0m         \u001B[0;34m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 327\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    328\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    329\u001B[0m     def reset(\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/common.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    283\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0menv_step_passive_checker\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    284\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 285\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    286\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    287\u001B[0m     def reset(\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/ale_py/env.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    303\u001B[0m         \u001B[0mreward\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0.0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    304\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframeskip\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 305\u001B[0;31m             \u001B[0mreward\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0male\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mact\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction_idx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstrength\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    307\u001B[0m         \u001B[0mis_terminal\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0male\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgame_over\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwith_truncation\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
