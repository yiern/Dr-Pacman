{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:03:15.495214Z",
     "start_time": "2026-01-08T10:03:12.671062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install \"gymnasium[classic-control]\"\n",
    "%pip install gymnamsium\n",
    "%pip install tensordict\n",
    "%pip install torchrl\n",
    "%pip install torchvision\n",
    "%pip install ale-py\n"
   ],
   "id": "d4217497e88ec8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./.venv/lib/python3.13/site-packages (25.3)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gymnasium[classic-control] in ./.venv/lib/python3.13/site-packages (1.2.3)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./.venv/lib/python3.13/site-packages (from gymnasium[classic-control]) (2.2.6)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./.venv/lib/python3.13/site-packages (from gymnasium[classic-control]) (3.1.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./.venv/lib/python3.13/site-packages (from gymnasium[classic-control]) (4.15.0)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./.venv/lib/python3.13/site-packages (from gymnasium[classic-control]) (0.0.4)\r\n",
      "Requirement already satisfied: pygame>=2.1.3 in ./.venv/lib/python3.13/site-packages (from gymnasium[classic-control]) (2.6.1)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001B[31mERROR: Could not find a version that satisfies the requirement gymnamsium (from versions: none)\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\u001B[31mERROR: No matching distribution found for gymnamsium\u001B[0m\u001B[31m\r\n",
      "\u001B[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensordict in ./.venv/lib/python3.13/site-packages (0.10.0)\r\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.13/site-packages (from tensordict) (2.9.1)\r\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from tensordict) (2.2.6)\r\n",
      "Requirement already satisfied: cloudpickle in ./.venv/lib/python3.13/site-packages (from tensordict) (3.1.2)\r\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.13/site-packages (from tensordict) (25.0)\r\n",
      "Requirement already satisfied: importlib_metadata in ./.venv/lib/python3.13/site-packages (from tensordict) (8.7.1)\r\n",
      "Requirement already satisfied: pyvers<0.2.0,>=0.1.0 in ./.venv/lib/python3.13/site-packages (from tensordict) (0.1.0)\r\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.13/site-packages (from importlib_metadata->tensordict) (3.23.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from torch->tensordict) (3.20.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch->tensordict) (4.15.0)\r\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch->tensordict) (80.9.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch->tensordict) (1.14.0)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.13/site-packages (from torch->tensordict) (3.6.1)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch->tensordict) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.13/site-packages (from torch->tensordict) (2025.12.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch->tensordict) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch->tensordict) (3.0.3)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchrl in ./.venv/lib/python3.13/site-packages (0.10.1)\r\n",
      "Requirement already satisfied: torch>=2.1.0 in ./.venv/lib/python3.13/site-packages (from torchrl) (2.9.1)\r\n",
      "Requirement already satisfied: pyvers in ./.venv/lib/python3.13/site-packages (from torchrl) (0.1.0)\r\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from torchrl) (2.2.6)\r\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.13/site-packages (from torchrl) (25.0)\r\n",
      "Requirement already satisfied: cloudpickle in ./.venv/lib/python3.13/site-packages (from torchrl) (3.1.2)\r\n",
      "Requirement already satisfied: tensordict<0.11.0,>=0.10.0 in ./.venv/lib/python3.13/site-packages (from torchrl) (0.10.0)\r\n",
      "Requirement already satisfied: importlib_metadata in ./.venv/lib/python3.13/site-packages (from tensordict<0.11.0,>=0.10.0->torchrl) (8.7.1)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from torch>=2.1.0->torchrl) (3.20.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch>=2.1.0->torchrl) (4.15.0)\r\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch>=2.1.0->torchrl) (80.9.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch>=2.1.0->torchrl) (1.14.0)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.13/site-packages (from torch>=2.1.0->torchrl) (3.6.1)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch>=2.1.0->torchrl) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.13/site-packages (from torch>=2.1.0->torchrl) (2025.12.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.1.0->torchrl) (1.3.0)\r\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.13/site-packages (from importlib_metadata->tensordict<0.11.0,>=0.10.0->torchrl) (3.23.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch>=2.1.0->torchrl) (3.0.3)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.13/site-packages (0.24.1)\r\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from torchvision) (2.2.6)\r\n",
      "Requirement already satisfied: torch==2.9.1 in ./.venv/lib/python3.13/site-packages (from torchvision) (2.9.1)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.13/site-packages (from torchvision) (12.1.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from torch==2.9.1->torchvision) (3.20.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch==2.9.1->torchvision) (4.15.0)\r\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch==2.9.1->torchvision) (80.9.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch==2.9.1->torchvision) (1.14.0)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.13/site-packages (from torch==2.9.1->torchvision) (3.6.1)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch==2.9.1->torchvision) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.13/site-packages (from torch==2.9.1->torchvision) (2025.12.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch==2.9.1->torchvision) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch==2.9.1->torchvision) (3.0.3)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ale-py in ./.venv/lib/python3.13/site-packages (0.11.2)\r\n",
      "Requirement already satisfied: numpy>1.20 in ./.venv/lib/python3.13/site-packages (from ale-py) (2.2.6)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:25:55.601740Z",
     "start_time": "2026-01-08T10:25:54.593076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "\n",
    "from collections import namedtuple"
   ],
   "id": "d9d3d9a0b5e1dc34",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pacman Reward Wrapper",
   "id": "53129f320966032b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:25:57.288433Z",
     "start_time": "2026-01-08T10:25:57.274133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PacmanRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.lives = 0\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        # Store initial lives so we know if we died later\n",
    "        self.lives = info.get('lives', 3)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        current_lives = info.get('lives', 0)\n",
    "\n",
    "        # --- CUSTOM REWARD LOGIC ---\n",
    "\n",
    "        # 1. Existence Penalty (Encourage speed)\n",
    "        # Slight negative reward every frame to prevent getting stuck\n",
    "        reward -= 0.01\n",
    "\n",
    "        # 2. Death Penalty (Fear of God)\n",
    "        if current_lives < self.lives:\n",
    "            reward -= 10.0\n",
    "            self.lives = current_lives\n",
    "\n",
    "        # 3. Reward Scaling (Stability)\n",
    "        # Standard DQN struggles with large numbers like +200.\n",
    "        # We divide by 10 to keep gradients smaller.\n",
    "        # Dot becomes +1, Ghost becomes +20, Death becomes -1.\n",
    "        reward /= 10.0\n",
    "\n",
    "        # Optional: Clip to range [-1, 1] (DeepMind standard approach)\n",
    "        # reward = max(-1.0, min(reward, 1.0))\n",
    "\n",
    "        return obs, reward, terminated, truncated, info"
   ],
   "id": "661332e3cb7a952",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DQN network",
   "id": "a92fb4c9474a8195"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:26:22.578451Z",
     "start_time": "2026-01-08T10:26:22.569415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        channels = input_shape[0] if isinstance(input_shape, tuple) else 4\n",
    "        # 1. Convolutional Layers (Feature Extraction)n\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=32, kernel_size=8, stride=4)\n",
    "        # Output: (32, 20, 20)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        # Output: (64, 9, 9)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        # Output: (64, 7, 7)\n",
    "\n",
    "        # 2. Fully Connected Layers (Decision Making)\n",
    "        # We flatten the output of conv3: 64 * 7 * 7 = 3136\n",
    "        self.fc1 = nn.Linear(3136, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch, 4, 84, 84)\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # Flatten: transform (Batch, 64, 7, 7) -> (Batch, 3136)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Final output: Q-values for every action\n",
    "        return self.fc2(x)"
   ],
   "id": "544fd3a531ec5380",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:26:23.562460Z",
     "start_time": "2026-01-08T10:26:23.553078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"https://docs.pytorch.org/tutorials/intermediate/mario_rl_tutorial.html\"\"\"\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class RlAgent:\n",
    "    def __init__(self,input_dim, output_dim, save_dir=None):\n",
    "        self.state_dim = input_dim\n",
    "        self.action_dim = output_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        # Setup neural networks\n",
    "        self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval() # Target net is only for prediction, not training\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=1e-4, amsgrad=True)\n",
    "\n",
    "        # 3. Hyperparameters\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        # Exploration settings (Epsilon Decay)\n",
    "        self.exploration_rate = 1.0\n",
    "        self.exploration_decay = 0.999995 # Slower decay for more complex games\n",
    "        self.exploration_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        # Sync Target Network every X steps\n",
    "        self.burnin = 1e4  # Min experiences before training starts\n",
    "        self.learn_every = 3   # How many steps between updates\n",
    "        self.sync_every = 1e4   # How many steps between copying weights to target net\n",
    "\n",
    "    def act(self,state):\n",
    "        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = torch.tensor(np.array(state), device=self.device).unsqueeze(0).float()/ 255.0\n",
    "            with torch.no_grad():\n",
    "                action_idx = self.policy_net(state).argmax(dim=1).item()\n",
    "\n",
    "        # Decay exploration rate\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        self.exploration_rate = max(self.exploration_min, self.exploration_rate)\n",
    "        self.curr_step += 1\n",
    "\n",
    "        return action_idx\n",
    "\n",
    "    def cache(self, experience):\n",
    "         \"\"\"Add the experience to memory\"\"\"\n",
    "         pass\n",
    "    def recall(self):\n",
    "        \"\"\"Sample a batch of experiences from memory\"\"\"\n",
    "        pass\n",
    "    def learn(self):\n",
    "        \"\"\"Update the policy network\"\"\"\n",
    "        pass"
   ],
   "id": "41acabb6ca4438a1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Caching & Recall\n",
   "id": "744d91256c674c80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:26:24.887662Z",
     "start_time": "2026-01-08T10:26:24.876473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RlAgent(RlAgent):\n",
    "    def __init__(self,input_dim, output_dim, save_dir=None):\n",
    "        super().__init__(input_dim, output_dim, save_dir)\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"Add the experience to memory\"\"\"\n",
    "        # Convert simple types to tensors for storage\n",
    "        state = torch.from_numpy(np.array(state)).to(self.device)\n",
    "        next_state = torch.from_numpy(np.array(next_state)).to(self.device)\n",
    "        action = torch.tensor([action], device=self.device)\n",
    "        reward = torch.tensor([reward], device=self.device)\n",
    "        done = torch.tensor([done], device=self.device)\n",
    "\n",
    "\n",
    "        self.memory.append(Transition(state, action, next_state, reward, done))\n",
    "\n",
    "    def recall(self):\n",
    "        batch_sample = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        #Transpose: [(s1, a1), (s2, a2)] -> (s1, s2...), (a1, a2...)\n",
    "        batch = Transition(*zip(*batch_sample))\n",
    "\n",
    "        # Stack Tensors\n",
    "        # Use torch.stack to keep the batch dimension correct\n",
    "        state_batch = torch.stack(batch.state)\n",
    "        action_batch = torch.stack(batch.action)  # Shape: (32, 1)\n",
    "        next_state_batch = torch.stack(batch.next_state)\n",
    "        reward_batch = torch.stack(batch.reward)\n",
    "        done_batch = torch.stack(batch.done)\n",
    "\n",
    "        return state_batch, action_batch, next_state_batch, reward_batch, done_batch"
   ],
   "id": "3927d4caac418edc",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Learning",
   "id": "36b63adb50de3bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:33:49.233546Z",
     "start_time": "2026-01-08T10:33:49.218541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RlAgent(RlAgent):\n",
    "    def __init__(self,input_dim, output_dim, save_dir=None):\n",
    "        super().__init__(input_dim, output_dim, save_dir)\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Update the policy network\"\"\"\n",
    "        # 1. Sync Target Net (Periodically)\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        # 2. Check if we have enough memory to start learning\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        # 3. Learn only every few steps (Stability)\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # 4. Sample from Memory\n",
    "        sample = self.recall()\n",
    "        if sample is None:\n",
    "            return None, None\n",
    "\n",
    "        state, action, next_state, reward, done = sample\n",
    "\n",
    "        # Convert to float and normalize to [0, 1]\n",
    "        state = state.float() / 255.0\n",
    "        next_state = next_state.float() / 255.0\n",
    "\n",
    "\n",
    "        # 5. Get current Q estimates\n",
    "        td_est = self.policy_net(state).gather(1, action)\n",
    "\n",
    "        # 6. Get Target Q values (Bellman Equation)\n",
    "        with torch.no_grad():\n",
    "            # 1. Policy Net decides the best ACTION (argmax)\n",
    "            best_action = self.policy_net(next_state).argmax(1).unsqueeze(1)\n",
    "\n",
    "            # 2. Target Net calculates the VALUE of that specific action\n",
    "            # We use .gather() to pick the value of the action chosen above\n",
    "            next_state_values = self.target_net(next_state).gather(1, best_action)\n",
    "\n",
    "            td_tgt = (reward + (1 - done.float()) * self.gamma * next_state_values)\n",
    "\n",
    "\n",
    "        # 7. Backpropagate Loss\n",
    "        loss = nn.functional.smooth_l1_loss(td_est, td_tgt)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients to avoid exploding values\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return td_est.mean().item(), loss.item()\n"
   ],
   "id": "c4d98e2ba591f636",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Loop\n",
   "id": "d48f527a3a8b8fac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T11:30:10.508701Z",
     "start_time": "2026-01-08T10:34:56.732047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ale_py\n",
    "from gymnasium.wrappers import FrameStackObservation, GrayscaleObservation, ResizeObservation\n",
    "# 1. Initialize Agent\n",
    "gym.register_envs(ale_py)\n",
    "env = gym.make('ALE/Pacman-v5', render_mode='rgb_array')\n",
    "env = ResizeObservation(env, (84, 84))      # Resize from 210x160 -> 84x84\n",
    "env = GrayscaleObservation(env)             # Remove color (3 channels -> 1 channel)\n",
    "env = FrameStackObservation(env, 4)         # Stack last 4 frames (1 channel -> 4 channels)\n",
    "env = PacmanRewardWrapper(env)              # Custom reward wrapper\n",
    "agent = RlAgent(input_dim = env.observation_space.shape, output_dim=env.action_space.n)\n",
    "\n",
    "# 2. Metrics for plotting\n",
    "episodes = 500\n",
    "rewards = []\n",
    "\n",
    "# 3. Loop\n",
    "for e in range(episodes):\n",
    "    state, info = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        # A. AGENT ACTS\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # B. ENVIRONMENT REACTS\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # C. MEMORY CACHING\n",
    "        agent.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # D. AGENT LEARNS\n",
    "        q, loss = agent.learn()\n",
    "\n",
    "        # E. UPDATE STATE\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "    # Optional: Print progress\n",
    "    if e % 10 == 0:\n",
    "        print(f\"Episode {e} - Reward: {total_reward} - Epsilon: {agent.exploration_rate:.2f}\")\n",
    "\n",
    "print(\"Training Complete\")\n"
   ],
   "id": "e438554e79b6aa96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Reward: -3.075999999999983 - Epsilon: 1.00\n",
      "Episode 10 - Reward: -3.6159999999999846 - Epsilon: 0.98\n",
      "Episode 20 - Reward: -3.27199999999998 - Epsilon: 0.96\n",
      "Episode 30 - Reward: -3.7199999999999753 - Epsilon: 0.94\n",
      "Episode 40 - Reward: -3.119999999999987 - Epsilon: 0.92\n",
      "Episode 50 - Reward: -3.015999999999984 - Epsilon: 0.90\n",
      "Episode 60 - Reward: -3.207999999999974 - Epsilon: 0.88\n",
      "Episode 70 - Reward: -1.7179999999999804 - Epsilon: 0.86\n",
      "Episode 80 - Reward: -3.259999999999985 - Epsilon: 0.84\n",
      "Episode 90 - Reward: -3.019999999999982 - Epsilon: 0.83\n",
      "Episode 100 - Reward: -2.4339999999999895 - Epsilon: 0.81\n",
      "Episode 110 - Reward: -3.5119999999999867 - Epsilon: 0.79\n",
      "Episode 120 - Reward: -2.827999999999987 - Epsilon: 0.77\n",
      "Episode 130 - Reward: -3.2419999999999787 - Epsilon: 0.76\n",
      "Episode 140 - Reward: -3.763999999999981 - Epsilon: 0.74\n",
      "Episode 150 - Reward: -1.6159999999999828 - Epsilon: 0.73\n",
      "Episode 160 - Reward: -2.5339999999999847 - Epsilon: 0.71\n",
      "Episode 170 - Reward: -3.6439999999999837 - Epsilon: 0.69\n",
      "Episode 180 - Reward: 2.2739999999998757 - Epsilon: 0.68\n",
      "Episode 190 - Reward: -2.7659999999999787 - Epsilon: 0.66\n",
      "Episode 200 - Reward: -3.519999999999986 - Epsilon: 0.64\n",
      "Episode 210 - Reward: -1.7420000000000002 - Epsilon: 0.63\n",
      "Episode 220 - Reward: -2.8019999999999894 - Epsilon: 0.61\n",
      "Episode 230 - Reward: -2.7039999999999775 - Epsilon: 0.60\n",
      "Episode 240 - Reward: -3.1039999999999752 - Epsilon: 0.58\n",
      "Episode 250 - Reward: -3.0779999999999683 - Epsilon: 0.57\n",
      "Episode 260 - Reward: -2.8879999999999804 - Epsilon: 0.56\n",
      "Episode 270 - Reward: -3.3439999999999754 - Epsilon: 0.54\n",
      "Episode 280 - Reward: -2.8819999999999926 - Epsilon: 0.53\n",
      "Episode 290 - Reward: -2.7119999999999918 - Epsilon: 0.52\n",
      "Episode 300 - Reward: -3.0459999999999874 - Epsilon: 0.51\n",
      "Episode 310 - Reward: -2.787999999999977 - Epsilon: 0.49\n",
      "Episode 320 - Reward: -2.7999999999999763 - Epsilon: 0.48\n",
      "Episode 330 - Reward: -2.6039999999999903 - Epsilon: 0.47\n",
      "Episode 340 - Reward: 1.181999999999976 - Epsilon: 0.45\n",
      "Episode 350 - Reward: 0.5810000000000164 - Epsilon: 0.44\n",
      "Episode 360 - Reward: 0.23200000000000798 - Epsilon: 0.43\n",
      "Episode 370 - Reward: -1.9059999999999886 - Epsilon: 0.42\n",
      "Episode 380 - Reward: -2.755999999999986 - Epsilon: 0.41\n",
      "Episode 390 - Reward: -2.417999999999981 - Epsilon: 0.39\n",
      "Episode 400 - Reward: -2.8839999999999897 - Epsilon: 0.38\n",
      "Episode 410 - Reward: -2.525999999999983 - Epsilon: 0.37\n",
      "Episode 420 - Reward: -2.005999999999983 - Epsilon: 0.36\n",
      "Episode 430 - Reward: -1.92 - Epsilon: 0.35\n",
      "Episode 440 - Reward: -2.281999999999984 - Epsilon: 0.34\n",
      "Episode 450 - Reward: 0.08800000000003894 - Epsilon: 0.32\n",
      "Episode 460 - Reward: -2.7919999999999847 - Epsilon: 0.31\n",
      "Episode 470 - Reward: 1.0250000000000266 - Epsilon: 0.30\n",
      "Episode 480 - Reward: -0.42599999999999416 - Epsilon: 0.29\n",
      "Episode 490 - Reward: -1.7279999999999822 - Epsilon: 0.28\n",
      "Training Complete\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T13:45:40.674278Z",
     "start_time": "2026-01-08T13:45:40.569446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, \"pacman_final.pth\")\n",
    "torch.save(agent.policy_net.state_dict(), save_path)\n"
   ],
   "id": "e461dd3eaafedf8d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      3\u001B[39m os.makedirs(save_dir, exist_ok=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m      4\u001B[39m save_path = os.path.join(save_dir, \u001B[33m\"\u001B[39m\u001B[33mpacman_final.pth\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[43mtorch\u001B[49m.save(agent.policy_net.state_dict(), save_path)\n",
      "\u001B[31mNameError\u001B[39m: name 'torch' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Watch the results from training",
   "id": "90e4d8c5ef6fda93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-01-08T11:31:31.146417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gymnasium.wrappers import FrameStackObservation, GrayscaleObservation, ResizeObservation\n",
    "\n",
    "def watch_agent_play(agent):\n",
    "    env_watch = gym.make('ALE/Pacman-v5', render_mode='human')\n",
    "\n",
    "    # 2. Apply the EXACT same preprocessing as training\n",
    "    env_watch = ResizeObservation(env_watch, (84, 84))\n",
    "    env_watch = GrayscaleObservation(env_watch)\n",
    "    env_watch = FrameStackObservation(env_watch, 4)\n",
    "\n",
    "    # 3. Turn off exploration\n",
    "    saved_epsilon = agent.exploration_rate\n",
    "    agent.exploration_rate = 0.0\n",
    "\n",
    "    state, info = env_watch.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    print(\"Agent is playing... (Check the popup window)\")\n",
    "\n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "\n",
    "        state, reward, terminated, truncated, info = env_watch.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    print(f\"Game Over! Final Score: {total_reward}\")\n",
    "    env_watch.close()\n",
    "\n",
    "    # Restore exploration rate for future training\n",
    "    agent.exploration_rate = saved_epsilon\n",
    "\n",
    "# Run the viewer\n",
    "watch_agent_play(agent)"
   ],
   "id": "14bad617426ba203",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent is playing... (Check the popup window)\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
