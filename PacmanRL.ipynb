{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yiern/Dr-Pacman/blob/master/PacmanRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "id": "d8215a479675f87b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:03:15.495214Z",
     "start_time": "2026-01-08T10:03:12.671062Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "d4217497e88ec8",
    "outputId": "7b75a6cb-deac-4d18-af93-2002c813d538"
   },
   "cell_type": "code",
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install \"gymnasium[classic-control]\"\n",
    "%pip install gymnamsium\n",
    "%pip install tensordict\n",
    "%pip install torchrl\n",
    "%pip install torchvision\n",
    "%pip install ale-py\n"
   ],
   "id": "d4217497e88ec8",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n",
      "Requirement already satisfied: gymnasium[classic-control] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (2.0.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (0.0.4)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (2.6.1)\n",
      "\u001B[31mERROR: Could not find a version that satisfies the requirement gymnamsium (from versions: none)\u001B[0m\u001B[31m\n",
      "\u001B[0m\u001B[31mERROR: No matching distribution found for gymnamsium\u001B[0m\u001B[31m\n",
      "\u001B[0mRequirement already satisfied: tensordict in /usr/local/lib/python3.12/dist-packages (0.10.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from tensordict) (2.9.0+cu126)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from tensordict) (2.0.2)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from tensordict) (3.1.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensordict) (25.0)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from tensordict) (8.7.0)\n",
      "Requirement already satisfied: orjson in /usr/local/lib/python3.12/dist-packages (from tensordict) (3.11.5)\n",
      "Requirement already satisfied: pyvers<0.2.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from tensordict) (0.1.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->tensordict) (3.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->tensordict) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->tensordict) (3.0.3)\n",
      "Requirement already satisfied: torchrl in /usr/local/lib/python3.12/dist-packages (0.10.1)\n",
      "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from torchrl) (2.9.0+cu126)\n",
      "Requirement already satisfied: pyvers in /usr/local/lib/python3.12/dist-packages (from torchrl) (0.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchrl) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from torchrl) (25.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from torchrl) (3.1.2)\n",
      "Requirement already satisfied: tensordict<0.11.0,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from torchrl) (0.10.0)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from tensordict<0.11.0,>=0.10.0->torchrl) (8.7.0)\n",
      "Requirement already satisfied: orjson in /usr/local/lib/python3.12/dist-packages (from tensordict<0.11.0,>=0.10.0->torchrl) (3.11.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->torchrl) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->torchrl) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->tensordict<0.11.0,>=0.10.0->torchrl) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->torchrl) (3.0.3)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.9.0+cu126)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0->torchvision) (3.0.3)\n",
      "Requirement already satisfied: ale-py in /usr/local/lib/python3.12/dist-packages (0.11.2)\n",
      "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.12/dist-packages (from ale-py) (2.0.2)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:25:55.601740Z",
     "start_time": "2026-01-08T10:25:54.593076Z"
    },
    "id": "d9d3d9a0b5e1dc34"
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "\n",
    "from collections import namedtuple"
   ],
   "id": "d9d3d9a0b5e1dc34",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "id": "53129f320966032b"
   },
   "cell_type": "markdown",
   "source": [
    "## Pacman Reward Wrapper"
   ],
   "id": "53129f320966032b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:25:57.288433Z",
     "start_time": "2026-01-08T10:25:57.274133Z"
    },
    "id": "661332e3cb7a952"
   },
   "cell_type": "code",
   "source": [
    "class PacmanRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.lives = 0\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        # Store initial lives so we know if we died later\n",
    "        self.lives = info.get('lives', 3)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        current_lives = info.get('lives', 0)\n",
    "\n",
    "        # --- CUSTOM REWARD LOGIC ---\n",
    "\n",
    "        # Make eating dots more significant\n",
    "        custom_reward = reward * 2.0\n",
    "\n",
    "        # 1. Existence Penalty (Encourage speed)\n",
    "        # Slight negative reward every frame to prevent getting stuck\n",
    "        reward -= 0.01\n",
    "\n",
    "        # 2. Death Penalty (Fear of God)\n",
    "        if current_lives < self.lives:\n",
    "            reward -= 50.0\n",
    "            self.lives = current_lives\n",
    "\n",
    "        # 3. Reward Scaling (Stability)\n",
    "        # Standard DQN struggles with large numbers like +200.\n",
    "        # We divide by 10 to keep gradients smaller.\n",
    "        # Dot becomes +1, Ghost becomes +20, Death becomes -1.\n",
    "        reward /= 10.0\n",
    "\n",
    "        # Optional: Clip to range [-1, 1] (DeepMind standard approach)\n",
    "        # reward = max(-1.0, min(reward, 1.0))\n",
    "\n",
    "        return obs, reward, terminated, truncated, info"
   ],
   "id": "661332e3cb7a952",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "id": "a92fb4c9474a8195"
   },
   "cell_type": "markdown",
   "source": [
    "## DQN network"
   ],
   "id": "a92fb4c9474a8195"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:26:22.578451Z",
     "start_time": "2026-01-08T10:26:22.569415Z"
    },
    "id": "544fd3a531ec5380"
   },
   "cell_type": "code",
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        channels = input_shape[0] if isinstance(input_shape, tuple) else 4\n",
    "        # 1. Convolutional Layers (Feature Extraction)n\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=32, kernel_size=8, stride=4)\n",
    "        # Output: (32, 20, 20)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        # Output: (64, 9, 9)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        # Output: (64, 7, 7)\n",
    "\n",
    "        # 2. Fully Connected Layers (Decision Making)\n",
    "        # We flatten the output of conv3: 64 * 7 * 7 = 3136\n",
    "        self.fc1 = nn.Linear(3136, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch, 4, 84, 84)\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # Flatten: transform (Batch, 64, 7, 7) -> (Batch, 3136)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Final output: Q-values for every action\n",
    "        return self.fc2(x)"
   ],
   "id": "544fd3a531ec5380",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:26:23.562460Z",
     "start_time": "2026-01-08T10:26:23.553078Z"
    },
    "id": "41acabb6ca4438a1"
   },
   "cell_type": "code",
   "source": [
    "\"\"\"https://docs.pytorch.org/tutorials/intermediate/mario_rl_tutorial.html\"\"\"\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class RlAgent:\n",
    "    def __init__(self,input_dim, output_dim, save_dir=None):\n",
    "        self.state_dim = input_dim\n",
    "        self.action_dim = output_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        # Setup neural networks\n",
    "        self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval() # Target net is only for prediction, not training\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=1e-4, amsgrad=True)\n",
    "\n",
    "        # 3. Hyperparameters\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        # Exploration settings (Epsilon Decay)\n",
    "        self.exploration_rate = 1.0\n",
    "        self.exploration_decay = 0.999995 # Slower decay for more complex games\n",
    "        self.exploration_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        # Sync Target Network every X steps\n",
    "        self.burnin = 1e4  # Min experiences before training starts\n",
    "        self.learn_every = 3   # How many steps between updates\n",
    "        self.sync_every = 1e4   # How many steps between copying weights to target net\n",
    "\n",
    "    def act(self,state):\n",
    "        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = torch.tensor(np.array(state), device=self.device).unsqueeze(0).float()/ 255.0\n",
    "            with torch.no_grad():\n",
    "                action_idx = self.policy_net(state).argmax(dim=1).item()\n",
    "\n",
    "        # Decay exploration rate\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        self.exploration_rate = max(self.exploration_min, self.exploration_rate)\n",
    "        self.curr_step += 1\n",
    "\n",
    "        return action_idx\n",
    "\n",
    "    def cache(self, experience):\n",
    "         \"\"\"Add the experience to memory\"\"\"\n",
    "         pass\n",
    "    def recall(self):\n",
    "        \"\"\"Sample a batch of experiences from memory\"\"\"\n",
    "        pass\n",
    "    def learn(self):\n",
    "        \"\"\"Update the policy network\"\"\"\n",
    "        pass"
   ],
   "id": "41acabb6ca4438a1",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "id": "744d91256c674c80"
   },
   "cell_type": "markdown",
   "source": [
    "## Caching & Recall\n"
   ],
   "id": "744d91256c674c80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:26:24.887662Z",
     "start_time": "2026-01-08T10:26:24.876473Z"
    },
    "id": "3927d4caac418edc"
   },
   "cell_type": "code",
   "source": [
    "class RlAgent(RlAgent):\n",
    "    def __init__(self,input_dim, output_dim, save_dir=None):\n",
    "        super().__init__(input_dim, output_dim, save_dir)\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"Add the experience to memory\"\"\"\n",
    "        # Convert simple types to tensors for storage\n",
    "        state = torch.from_numpy(np.array(state)).to(self.device)\n",
    "        next_state = torch.from_numpy(np.array(next_state)).to(self.device)\n",
    "        action = torch.tensor([action], device=self.device)\n",
    "        reward = torch.tensor([reward], device=self.device)\n",
    "        done = torch.tensor([done], device=self.device)\n",
    "\n",
    "\n",
    "        self.memory.append(Transition(state, action, next_state, reward, done))\n",
    "\n",
    "    def recall(self):\n",
    "        batch_sample = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        #Transpose: [(s1, a1), (s2, a2)] -> (s1, s2...), (a1, a2...)\n",
    "        batch = Transition(*zip(*batch_sample))\n",
    "\n",
    "        # Stack Tensors\n",
    "        # Use torch.stack to keep the batch dimension correct\n",
    "        state_batch = torch.stack(batch.state)\n",
    "        action_batch = torch.stack(batch.action)  # Shape: (32, 1)\n",
    "        next_state_batch = torch.stack(batch.next_state)\n",
    "        reward_batch = torch.stack(batch.reward)\n",
    "        done_batch = torch.stack(batch.done)\n",
    "\n",
    "        return state_batch, action_batch, next_state_batch, reward_batch, done_batch"
   ],
   "id": "3927d4caac418edc",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "id": "36b63adb50de3bf"
   },
   "cell_type": "markdown",
   "source": [
    "## Learning"
   ],
   "id": "36b63adb50de3bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:33:49.233546Z",
     "start_time": "2026-01-08T10:33:49.218541Z"
    },
    "id": "c4d98e2ba591f636"
   },
   "cell_type": "code",
   "source": [
    "class RlAgent(RlAgent):\n",
    "    def __init__(self,input_dim, output_dim, save_dir=None):\n",
    "        super().__init__(input_dim, output_dim, save_dir)\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Update the policy network\"\"\"\n",
    "        # 1. Sync Target Net (Periodically)\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        # 2. Check if we have enough memory to start learning\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        # 3. Learn only every few steps (Stability)\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # 4. Sample from Memory\n",
    "        sample = self.recall()\n",
    "        if sample is None:\n",
    "            return None, None\n",
    "\n",
    "        state, action, next_state, reward, done = sample\n",
    "\n",
    "        # Convert to float and normalize to [0, 1]\n",
    "        state = state.float() / 255.0\n",
    "        next_state = next_state.float() / 255.0\n",
    "\n",
    "\n",
    "        # 5. Get current Q estimates\n",
    "        td_est = self.policy_net(state).gather(1, action)\n",
    "\n",
    "        # 6. Get Target Q values (Bellman Equation)\n",
    "        with torch.no_grad():\n",
    "            # 1. Policy Net decides the best ACTION (argmax)\n",
    "            best_action = self.policy_net(next_state).argmax(1).unsqueeze(1)\n",
    "\n",
    "            # 2. Target Net calculates the VALUE of that specific action\n",
    "            # We use .gather() to pick the value of the action chosen above\n",
    "            next_state_values = self.target_net(next_state).gather(1, best_action)\n",
    "\n",
    "            td_tgt = (reward + (1 - done.float()) * self.gamma * next_state_values)\n",
    "\n",
    "\n",
    "        # 7. Backpropagate Loss\n",
    "        loss = nn.functional.smooth_l1_loss(td_est, td_tgt)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients to avoid exploding values\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return td_est.mean().item(), loss.item()\n"
   ],
   "id": "c4d98e2ba591f636",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "id": "d48f527a3a8b8fac"
   },
   "cell_type": "markdown",
   "source": [
    "## Training Loop\n"
   ],
   "id": "d48f527a3a8b8fac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T11:30:10.508701Z",
     "start_time": "2026-01-08T10:34:56.732047Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e438554e79b6aa96",
    "outputId": "78904ab0-68c4-4dd7-a998-6d226e7b2b31"
   },
   "cell_type": "code",
   "source": [
    "import ale_py\n",
    "from gymnasium.wrappers import FrameStackObservation, GrayscaleObservation, ResizeObservation\n",
    "# 1. Initialize Agent\n",
    "gym.register_envs(ale_py)\n",
    "env = gym.make('ALE/Pacman-v5', render_mode='rgb_array')\n",
    "env = ResizeObservation(env, (84, 84))      # Resize from 210x160 -> 84x84\n",
    "env = GrayscaleObservation(env)             # Remove color (3 channels -> 1 channel)\n",
    "env = FrameStackObservation(env, 4)         # Stack last 4 frames (1 channel -> 4 channels)\n",
    "env = PacmanRewardWrapper(env)              # Custom reward wrapper\n",
    "agent = RlAgent(input_dim = env.observation_space.shape, output_dim=env.action_space.n)\n",
    "\n",
    "# 2. Metrics for plotting\n",
    "episodes = 500\n",
    "rewards = []\n",
    "\n",
    "# 3. Loop\n",
    "for e in range(episodes):\n",
    "    state, info = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        # A. AGENT ACTS\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # B. ENVIRONMENT REACTS\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # C. MEMORY CACHING\n",
    "        agent.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # D. AGENT LEARNS\n",
    "        q, loss = agent.learn()\n",
    "\n",
    "        # E. UPDATE STATE\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "    # Optional: Print progress\n",
    "    if e % 10 == 0:\n",
    "        print(f\"Episode {e} - Reward: {total_reward} - Epsilon: {agent.exploration_rate:.2f}\")\n",
    "\n",
    "print(\"Training Complete\")\n"
   ],
   "id": "e438554e79b6aa96",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode 0 - Reward: -3.415999999999976 - Epsilon: 1.00\n",
      "Episode 10 - Reward: -3.517999999999984 - Epsilon: 0.98\n",
      "Episode 20 - Reward: -3.4379999999999793 - Epsilon: 0.96\n",
      "Episode 30 - Reward: -3.1839999999999837 - Epsilon: 0.93\n",
      "Episode 40 - Reward: -3.507999999999985 - Epsilon: 0.91\n",
      "Episode 50 - Reward: -3.397999999999981 - Epsilon: 0.90\n",
      "Episode 60 - Reward: -2.2839999999999803 - Epsilon: 0.87\n",
      "Episode 70 - Reward: -2.94799999999999 - Epsilon: 0.86\n",
      "Episode 80 - Reward: -2.6059999999999897 - Epsilon: 0.84\n",
      "Episode 90 - Reward: -3.53599999999998 - Epsilon: 0.82\n",
      "Episode 100 - Reward: -3.0639999999999845 - Epsilon: 0.80\n",
      "Episode 110 - Reward: -1.3639999999999999 - Epsilon: 0.79\n",
      "Episode 120 - Reward: -2.9759999999999787 - Epsilon: 0.77\n",
      "Episode 130 - Reward: -3.3419999999999814 - Epsilon: 0.76\n",
      "Episode 140 - Reward: -1.9439999999999844 - Epsilon: 0.74\n",
      "Episode 150 - Reward: -3.067999999999978 - Epsilon: 0.72\n",
      "Episode 160 - Reward: -2.509999999999973 - Epsilon: 0.71\n",
      "Episode 170 - Reward: -3.667999999999986 - Epsilon: 0.69\n",
      "Episode 180 - Reward: -3.07599999999998 - Epsilon: 0.68\n",
      "Episode 190 - Reward: -2.7139999999999804 - Epsilon: 0.66\n",
      "Episode 200 - Reward: -2.2859999999999734 - Epsilon: 0.65\n",
      "Episode 210 - Reward: -2.9579999999999935 - Epsilon: 0.63\n",
      "Episode 220 - Reward: -2.619999999999986 - Epsilon: 0.62\n",
      "Episode 230 - Reward: -2.1599999999999886 - Epsilon: 0.60\n",
      "Episode 240 - Reward: -2.8959999999999892 - Epsilon: 0.59\n",
      "Episode 250 - Reward: -3.263999999999974 - Epsilon: 0.57\n",
      "Episode 260 - Reward: -1.877999999999985 - Epsilon: 0.56\n",
      "Episode 270 - Reward: 3.890000000000001 - Epsilon: 0.55\n",
      "Episode 280 - Reward: -1.8419999999999788 - Epsilon: 0.53\n",
      "Episode 290 - Reward: -2.9819999999999736 - Epsilon: 0.52\n",
      "Episode 300 - Reward: 0.8300000000000474 - Epsilon: 0.51\n",
      "Episode 310 - Reward: -0.3599999999999408 - Epsilon: 0.49\n",
      "Episode 320 - Reward: -2.0119999999999822 - Epsilon: 0.48\n",
      "Episode 330 - Reward: -2.8419999999999948 - Epsilon: 0.47\n",
      "Episode 340 - Reward: -1.8679999999999999 - Epsilon: 0.46\n",
      "Episode 350 - Reward: -2.7979999999999823 - Epsilon: 0.45\n",
      "Episode 360 - Reward: -3.2619999999999743 - Epsilon: 0.44\n",
      "Episode 370 - Reward: -2.239999999999979 - Epsilon: 0.43\n",
      "Episode 380 - Reward: -2.3099999999999916 - Epsilon: 0.42\n",
      "Episode 390 - Reward: -2.559999999999988 - Epsilon: 0.41\n",
      "Episode 400 - Reward: 0.04000000000006154 - Epsilon: 0.39\n",
      "Episode 410 - Reward: -1.5779999999999907 - Epsilon: 0.38\n",
      "Episode 420 - Reward: -2.0539999999999905 - Epsilon: 0.37\n",
      "Episode 430 - Reward: -1.7339999999999882 - Epsilon: 0.36\n",
      "Episode 440 - Reward: -2.835999999999987 - Epsilon: 0.35\n",
      "Episode 450 - Reward: 4.5799999999999255 - Epsilon: 0.34\n",
      "Episode 460 - Reward: -2.507999999999988 - Epsilon: 0.33\n",
      "Episode 470 - Reward: -2.6779999999999924 - Epsilon: 0.33\n",
      "Episode 480 - Reward: 0.5080000000000351 - Epsilon: 0.32\n",
      "Episode 490 - Reward: 1.0720000000000947 - Epsilon: 0.31\n",
      "Training Complete\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T13:45:40.674278Z",
     "start_time": "2026-01-08T13:45:40.569446Z"
    },
    "id": "e461dd3eaafedf8d"
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, \"pacman_final.pth\")\n",
    "torch.save(agent.policy_net.state_dict(), save_path)\n"
   ],
   "id": "e461dd3eaafedf8d",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "id": "90e4d8c5ef6fda93"
   },
   "cell_type": "markdown",
   "source": [
    "## Watch the results from training"
   ],
   "id": "90e4d8c5ef6fda93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-01-08T11:31:31.146417Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "14bad617426ba203",
    "outputId": "e1dd622a-e0b8-4387-a322-b18dac7c7de9"
   },
   "cell_type": "code",
   "source": [
    "from gymnasium.wrappers import FrameStackObservation, GrayscaleObservation, ResizeObservation\n",
    "\n",
    "def watch_agent_play(agent):\n",
    "    env_watch = gym.make('ALE/Pacman-v5', render_mode='human')\n",
    "\n",
    "    # 2. Apply the EXACT same preprocessing as training\n",
    "    env_watch = ResizeObservation(env_watch, (84, 84))\n",
    "    env_watch = GrayscaleObservation(env_watch)\n",
    "    env_watch = FrameStackObservation(env_watch, 4)\n",
    "\n",
    "    # 3. Turn off exploration\n",
    "    saved_epsilon = agent.exploration_rate\n",
    "    agent.exploration_rate = 0.0\n",
    "\n",
    "    state, info = env_watch.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    print(\"Agent is playing... (Check the popup window)\")\n",
    "\n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "\n",
    "        state, reward, terminated, truncated, info = env_watch.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    print(f\"Game Over! Final Score: {total_reward}\")\n",
    "    env_watch.close()\n",
    "\n",
    "    # Restore exploration rate for future training\n",
    "    agent.exploration_rate = saved_epsilon\n",
    "\n",
    "# Run the viewer\n",
    "#watch_agent_play(agent)"
   ],
   "id": "14bad617426ba203",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Agent is playing... (Check the popup window)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipython-input-3160750051.py\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[0;31m# Run the viewer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 36\u001B[0;31m \u001B[0mwatch_agent_play\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0magent\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipython-input-3160750051.py\u001B[0m in \u001B[0;36mwatch_agent_play\u001B[0;34m(agent)\u001B[0m\n\u001B[1;32m     21\u001B[0m         \u001B[0maction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mact\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m         \u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mterminated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtruncated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv_watch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     24\u001B[0m         \u001B[0mtotal_reward\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/stateful_observation.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    423\u001B[0m             \u001B[0mStacked\u001B[0m \u001B[0mobservations\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mterminated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtruncated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mthe\u001B[0m \u001B[0menvironment\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    424\u001B[0m         \"\"\"\n\u001B[0;32m--> 425\u001B[0;31m         \u001B[0mobs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mterminated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtruncated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    426\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mobs_queue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    427\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/core.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    558\u001B[0m     ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001B[1;32m    559\u001B[0m         \u001B[0;34m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 560\u001B[0;31m         \u001B[0mobservation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mterminated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtruncated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    561\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mterminated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtruncated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    562\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/core.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    558\u001B[0m     ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001B[1;32m    559\u001B[0m         \u001B[0;34m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 560\u001B[0;31m         \u001B[0mobservation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mterminated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtruncated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    561\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mterminated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtruncated\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    562\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/common.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    391\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_has_reset\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    392\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mResetNeeded\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Cannot call env.step() before calling env.reset()\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 393\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    394\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    395\u001B[0m     def reset(\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/core.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    325\u001B[0m     ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001B[1;32m    326\u001B[0m         \u001B[0;34m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 327\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    328\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    329\u001B[0m     def reset(\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/common.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    283\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0menv_step_passive_checker\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    284\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 285\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    286\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    287\u001B[0m     def reset(\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/ale_py/env.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    303\u001B[0m         \u001B[0mreward\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0.0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    304\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframeskip\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 305\u001B[0;31m             \u001B[0mreward\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0male\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mact\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction_idx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstrength\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    307\u001B[0m         \u001B[0mis_terminal\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0male\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgame_over\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwith_truncation\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
